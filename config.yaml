# DocuGen Configuration
# =====================
# DocuGen uses LangChain for universal LLM provider support.
# Supports: Ollama (local), OpenRouter, Together AI, OpenAI, and any OpenAI-compatible API.

# Step 1: LLM Connection
# ----------------------
llm:
  # API endpoint for LLM inference (OpenAI-compatible format with /v1)
  base_url: "http://localhost:11434/v1"  # Local Ollama (default)
  # base_url: "https://openrouter.ai/api/v1"     # OpenRouter
  # base_url: "https://api.together.xyz/v1"      # Together AI
  # base_url: "https://api.openai.com/v1"        # OpenAI
  # base_url: "http://localhost:1234/v1"         # LM Studio

  # API key environment variable (leave null for local Ollama)
  api_key_env: null  # e.g., "OPENROUTER_API_KEY", "OPENAI_API_KEY"

  timeout: 1200  # Request timeout (seconds) - Large value for Layer 4 synthesis of entire codebase

# Step 2: Model Selection
# -----------------------
models:
  default: "mistral-nemo:latest"        # General purpose
  summarizer: "gemma3n:e4b"              # Layer 1 - Fast, high-level summaries
  detailing: "granite3.3:latest"        # Layer 2 - Detailed documentation (quality critical)
  relationship_mapper: "granite3.3:latest"  # Layer 3 - Dependency analysis (upgraded from qwen3:1.7b for better performance)
  documentation: "mistral-nemo:latest"  # Layer 4 - Final documentation synthesis (better at following formatting instructions)
  validation: "granite3.3:latest"       # Validation model (defaults to detailing if null)

# Step 3: Quality Validation
# ---------------------------
validation:
  max_iterations: 3                     # Refinement attempts before manual review (1-10)
  min_summary_length: 50                # Minimum summary length (characters)
  require_all_public_methods: true      # Enforce complete method documentation

# Step 4: Output Settings
# -----------------------
output:
  base_path: "./docs_output"            # Documentation output directory
  format: "markdown"                    # Output format
  include_metadata: true                # Include generation timestamps and validation status

# Step 5: Processing Options
# ---------------------------
processing:
  parallel_files: 1                     # Concurrent file processing (reduced to avoid timeouts with large models)
  enable_incremental: true              # Only re-process changed files (not yet implemented)
  supported_languages:
    - "csharp"

# Notes:
# - For local Ollama: Model names must exist (check with: ollama list)
# - For OpenRouter/OpenAI: Use provider/model format (e.g., "anthropic/claude-3-5-sonnet")
# - API keys should be stored in .env file and referenced via api_key_env
# - Memory usage scales with parallel_files and model size
# - For large codebases (5000+ files), reduce parallel_files to 2-4
# - Use verbose mode for debugging: docugen document -v
# - Test connection: docugen test-connection
